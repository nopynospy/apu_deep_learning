{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "LAB9_DCGAN_octopus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nopynospy/apu_deep_learning/blob/main/LAB9_DCGAN_octopus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VPaDbpGdnZV"
      },
      "source": [
        "**This exercise trains the GAN network with octopus images from Google Quickdraw and generates synthetic octopus images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nthXuhyF_05w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bb609f-226f-47a1-f9b1-4b84a9cc7a23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7OUBOSYdkmk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiqSgHT3MTQx"
      },
      "source": [
        "https://quickdraw.withgoogle.com/data/octopus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwuqsXaIQYvy"
      },
      "source": [
        "To download data: \n",
        "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/octopus.npy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXlipRQW2YhT"
      },
      "source": [
        "Or, you may use this link: https://drive.google.com/file/d/1rnPf_2boiygqbwumc3KNlWt5zZKibgRU/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTxGZgxTMTQy"
      },
      "source": [
        "# This code runs Keras with tensorflow running in the background\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Activation, Conv2D, AveragePooling2D, Reshape, Dense, BatchNormalization, Dropout, Flatten, UpSampling2D, Conv2DTranspose\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.initializers import TruncatedNormal\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OmS2hnHM7Ma"
      },
      "source": [
        "input_images = \"/content/drive/MyDrive/gan/octopus.npy\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iQV_TaWNRiw"
      },
      "source": [
        "data = np.load(input_images)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLrQo3sIOMcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5105da-f188-4910-a67b-3193732765ab"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150152, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe6IBBQ9GHPH",
        "outputId": "3c525fbf-54f3-4705-99b9-9024e6bb1ed6"
      },
      "source": [
        "data.shape[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150152"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaIqtHRtMTQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bcf1e5-1ecb-4ab9-c2fb-50f0a813e45d"
      },
      "source": [
        "data = data/255\n",
        "data = np.reshape(data,(data.shape[0],28,28,1))\n",
        "img_w,img_h = data.shape[1:3]\n",
        "data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150152, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83bBhX_HMTQ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "dde29eec-fb73-45b2-fdf6-9e90f82f774a"
      },
      "source": [
        "n = np.random.randint(data.shape[0]) # randomly select an image\n",
        "print(n)\n",
        "sample_image = data[n]\n",
        "plt.imshow(sample_image[:,:,0], cmap='Greys')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47347\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f634c8aa490>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJz4HEVvcls0"
      },
      "source": [
        "Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0jUNbgMTRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdcf3d0-1ea9-41b0-bed4-ab6f915aa7f3"
      },
      "source": [
        "def discriminator_builder(depth=64,p=0.4): # depth - number of filters, p - coeff for dropout\n",
        "    \n",
        "    # Define inputs\n",
        "    inputs = Input((img_w,img_h,1))\n",
        "    \n",
        "    # Convolutional layers\n",
        "    conv1 = Conv2D(depth*1, 5, strides=2, padding='same', activation='relu')(inputs)\n",
        "    conv1 = Dropout(p)(conv1)\n",
        "    \n",
        "    conv2 = Conv2D(depth*2, 5, strides=2, padding='same', activation='relu')(conv1)\n",
        "    conv2 = Dropout(p)(conv2)\n",
        "    \n",
        "    conv3 = Conv2D(depth*4, 5, strides=2, padding='same', activation='relu')(conv2)\n",
        "    conv3 = Dropout(p)(conv3)\n",
        "    \n",
        "    conv4 = Conv2D(depth*8, 5, strides=1, padding='same', activation='relu')(conv3)\n",
        "    conv4 = Flatten()(Dropout(p)(conv4))\n",
        "    \n",
        "    output = Dense(1, activation='sigmoid')(conv4)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "discriminator = discriminator_builder()\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8), metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 256)         819456    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,311,553\n",
            "Trainable params: 4,311,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh11dVIocyBf"
      },
      "source": [
        "Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yB6dPP7z2aa"
      },
      "source": [
        "Build a Generator Model\n",
        "Stack of BN-ReLU-Conv2DTranpose to generate fake images\n",
        "Output activation is sigmoid Sigmoid converges easily. instead of tanh in [1].\n",
        "Arguments:\n",
        "inputs (Layer): Input layer of the generator\n",
        "the z-vector)\n",
        "image_size (tensor): Target size of one side\n",
        "(assuming square image)\n",
        "Return generator (Model): Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5QzLmHNMTRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f308caf2-cb17-4ba6-a786-eb858d742804"
      },
      "source": [
        "def generator_builder(z_dim=100,depth=64,p=0.4):\n",
        "    \n",
        "    # Define inputs\n",
        "    inputs = Input((z_dim,))\n",
        "    \n",
        "    # First dense layer\n",
        "    dense1 = Dense(7*7*64)(inputs)\n",
        "    dense1 = BatchNormalization(axis=-1,momentum=0.9)(dense1)\n",
        "    dense1 = Activation(activation='relu')(dense1)\n",
        "    dense1 = Reshape((7,7,64))(dense1)\n",
        "    dense1 = Dropout(p)(dense1)\n",
        "    \n",
        "    # Convolutional layers\n",
        "    conv1 = UpSampling2D()(dense1)\n",
        "    conv1 = Conv2DTranspose(int(depth/2), kernel_size=5, padding='same', activation=None,)(conv1)\n",
        "    conv1 = BatchNormalization(axis=-1,momentum=0.9)(conv1)\n",
        "    conv1 = Activation(activation='relu')(conv1)\n",
        "    \n",
        "    conv2 = UpSampling2D()(conv1)\n",
        "    conv2 = Conv2DTranspose(int(depth/4), kernel_size=5, padding='same', activation=None,)(conv2)\n",
        "    conv2 = BatchNormalization(axis=-1,momentum=0.9)(conv2)\n",
        "    conv2 = Activation(activation='relu')(conv2)\n",
        "    \n",
        "    #conv3 = UpSampling2D()(conv2)\n",
        "    conv3 = Conv2DTranspose(int(depth/8), kernel_size=5, padding='same', activation=None,)(conv2)\n",
        "    conv3 = BatchNormalization(axis=-1,momentum=0.9)(conv3)\n",
        "    conv3 = Activation(activation='relu')(conv3)\n",
        "\n",
        "    # Define output layers\n",
        "    output = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(conv3)\n",
        "\n",
        "    # Model definition    \n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.summary()\n",
        "   \n",
        "    return model\n",
        "\n",
        "generator = generator_builder()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3136)              316736    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 3136)             12544     \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 3136)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 32)       51232     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 14, 14, 32)        0         \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 28, 28, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 16)       12816     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 28, 28, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 28, 28, 16)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 8)        3208      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 28, 28, 8)        32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 28, 28, 8)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 1)         201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396,961\n",
            "Trainable params: 390,577\n",
            "Non-trainable params: 6,384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZIjuTBac7QF"
      },
      "source": [
        "Connecting the Adversaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd7CmAxBMTRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea06420-3c6e-42f1-9855-513974719a9d"
      },
      "source": [
        "def adversarial_builder(z_dim=100):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8), metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "adversarial_model = adversarial_builder()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model_1 (Functional)        (None, 28, 28, 1)         396961    \n",
            "                                                                 \n",
            " model (Functional)          (None, 1)                 4311553   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,708,514\n",
            "Trainable params: 4,702,130\n",
            "Non-trainable params: 6,384\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKfsevsddEL2"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvda93WfMTRe"
      },
      "source": [
        "# This is to set layer by layer of a net to be trainable or untrainable\n",
        "# apparently, it is necessary to it at individual layer level rather than the whole \"net\" level\n",
        "\n",
        "def make_trainable(net, val):\n",
        "    net.trainable = val\n",
        "    for l in net.layers:\n",
        "        l.trainable = val"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "zBrk9ztTMTRk"
      },
      "source": [
        "def train(epochs=2000,batch=128):\n",
        "    d_loss = []\n",
        "    a_loss = []\n",
        "    running_d_loss = 0\n",
        "    running_d_acc = 0\n",
        "    running_a_loss = 0\n",
        "    running_a_acc = 0\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        if i%100 == 0:\n",
        "            print(i) # print epoch number\n",
        "        # randomly pick real images from dataset\n",
        "        # data.shape[0] - Number of rows in data\n",
        "        real_imgs = np.reshape(data[np.random.choice(data.shape[0],batch,replace=False)],(batch,28,28,1))\n",
        "        # generate fake images using uniform random noise\n",
        "        fake_imgs = generator.predict(np.random.uniform(-1.0, 1.0, size=[batch, 100]))\n",
        "        \n",
        "        x = np.concatenate((real_imgs,fake_imgs))\n",
        "        \n",
        "        # label real and fake images\n",
        "        y = np.ones([2*batch,1]) # generates y with 256 (2x batch size) elements with value 1\n",
        "        y[batch:,:] = 0.  # converts the last 128(batch size) elements to 0.0\n",
        "       \n",
        "        make_trainable(discriminator, True) # First train the discriminator\n",
        "        # \"discriminator.train_on_batch\" Runs a single gradient update on a single batch of data\n",
        "        # Try typing the above in a new cell and teh function appears in a pop up\n",
        "        # or hover mouse on this function in the line below to see the pop up \n",
        "        # \"d_loss.append\" is for creating the sequence of losses as epochs progress. This is for plotting purposes\n",
        "        d_loss.append(discriminator.train_on_batch(x,y))\n",
        "        running_d_loss += d_loss[-1][0]\n",
        "        running_d_acc += d_loss[-1][1]\n",
        "\n",
        "        make_trainable(discriminator, False)  # Stop training the discriminator\n",
        "        # input to the adversarial model which is only geenrator now because discriminator not training\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch, 100])  \n",
        "        y = np.ones([batch,1])\n",
        "        a_loss.append(adversarial_model.train_on_batch(noise,y))\n",
        "        running_a_loss += a_loss[-1][0]\n",
        "        running_a_acc += a_loss[-1][1]\n",
        "        \n",
        "        if (i+1)%500 == 0:\n",
        "            print('Epoch #{}'.format(i+1))\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, running_d_loss/i, running_d_acc/i)\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, running_a_loss/i, running_a_acc/i)\n",
        "            print(log_mesg)\n",
        "            \n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "            gen_imgs = generator.predict(noise)\n",
        "            \n",
        "            plt.figure(figsize=(5,5))\n",
        "            \n",
        "            for k in range(gen_imgs.shape[0]):\n",
        "                plt.subplot(4, 4, k+1)\n",
        "                plt.imshow(gen_imgs[k, :, :, 0], cmap='gray')\n",
        "                plt.axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # plt.savefig('./images/run2_{}.png'.format(i+1))\n",
        "    return a_loss, d_loss\n",
        "        "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgtl_wtgGy9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157425f1-d17d-4538-fc85-f6ef711a2810"
      },
      "source": [
        "# To understand the command\n",
        "batch = 128\n",
        "real_imgs = np.reshape(data[np.random.choice(data.shape[0],batch,replace=False)],(batch,28,28,1))\n",
        "real_imgs.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSB0W9T9wagQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9fe0c4-7cd8-4e72-9596-995132852c74"
      },
      "source": [
        "a_loss_complete, d_loss_complete = train(epochs=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbXqcstdLiD"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0dOpaD-MTRr"
      },
      "source": [
        "ax = pd.DataFrame(\n",
        "    {\n",
        "        'Generative Loss': [loss[0] for loss in a_loss_complete],\n",
        "        'Discriminative Loss': [loss[0] for loss in d_loss_complete],\n",
        "    }\n",
        ").plot(title='Training loss', logy=True)\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrcVfylrMTRv"
      },
      "source": [
        "ax = pd.DataFrame(\n",
        "    {\n",
        "        'Generative Loss': [loss[1] for loss in a_loss_complete],\n",
        "        'Discriminative Loss': [loss[1] for loss in d_loss_complete],\n",
        "    }\n",
        ").plot(title='Training Accuracy')\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCMeRQd2WuFT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-yas36GW0Z0"
      },
      "source": [
        "# Tasks\n",
        "1. Add model visualization, necessary callback, model saving etc.\n",
        "2. Change hyper parameter settings and test performance\n",
        "3. Include tensorboard to monitor the training\n",
        "4. Extend the code to other images in google draw. "
      ]
    }
  ]
}